import pandas as pd


df_main = pd.read_csv('../data/raw/dating_app_behavior_dataset.csv')
df_ext = pd.read_csv('../data/raw/dating_app_behavior_dataset_extended1.csv')


df_main.head()


df_ext.head()


df_main.shape


df_ext.shape


df_main.columns


df_ext.columns


df_main.info()


df_ext.info()


df_ext['age'].describe()


df_main['last_active_hour'].describe()


df_main['swipe_right_ratio'].describe()


df_main['emoji_usage_rate'].describe()


cols_outliers = [
    "app_usage_time_min",
    "likes_received",
    "message_sent_count",
    "mutual_matches"
]


import matplotlib.pyplot as plt  # in order to see outliers and anomalies 

for col in cols_outliers:
    plt.figure()
    plt.boxplot(df_main[col], vert=False)
    plt.title(f"Boxplot of {col}")
    plt.xlabel(col)
    plt.show()


def detect_iqr_outliers(data, col):
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    return data[(data[col] < lower) | (data[col] > upper)]


for col in cols_outliers:
    outliers = detect_iqr_outliers(df_main, col)
    print(col, "outliers:", len(outliers))


common_cols = set(df_main.columns) & set(df_ext.columns)
only_in_main = set(df_main.columns) - set(df_ext.columns)
only_in_ext = set(df_ext.columns) - set(df_main.columns)

print("Common columns:", len(common_cols))
print("Only in df_main:", only_in_main)
print("Only in df_ext:", only_in_ext)


mismatch_cols = []

for col in common_cols:
    equal = df_main[col].equals(df_ext[col])
    if not equal:
        mismatch_cols.append(col)

mismatch_cols


# df_ext= df_main + new col


df_main.duplicated().sum()
df_ext.duplicated().sum()


# Separate numerical and categorical features


numerical_features = df_ext.select_dtypes(include=['float64', 'int64']).columns
features = df_ext[numerical_features]

categorical_features = df_ext.select_dtypes(include=['object']).columns
features = df_ext[categorical_features]


# one hot encoding or creating bins to normalize the interest tags given if our intent was to predict the dating outcome of people with similar interest


# high correlation (>0.80) towards the target --> drop columns in order to avoid overfitting


# numerical encoding --> numerical standardscaler 
# mean becomes 0 and their standard deviation becomes 1 --> 

import pandas as pd
from sklearn.preprocessing import StandardScaler



columns_to_scale = [
    'app_usage_time_min', 'swipe_right_ratio', 'likes_received', 'mutual_matches',
    'profile_pics_count', 'bio_length', 'message_sent_count', 'emoji_usage_rate',
    'last_active_hour', 'age', 'height_cm', 'weight_kg'
]

scaler = StandardScaler() # instance 

# Fit the scaler to the data
df_ext[columns_to_scale] = scaler.fit_transform(df_ext[columns_to_scale])


print(df_ext[columns_to_scale].mean())  # Should be approximately 0 - standardized measures 
print(df_ext[columns_to_scale].std())   # Should be approximately 1



